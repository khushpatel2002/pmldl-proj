
# Text Detoxification Using LoRA and PEFT

## Project Overview

This project aims to fine-tune Large Language Models (LLMs) using Low-Rank Adaptation (LoRA) and Parameter-Efficient Fine-Tuning (PEFT) techniques for **text detoxification**. The detoxification pipeline acts as a guardrail to ensure that any toxic content generated by AI agents is detected and neutralized before it reaches users.

The current progress includes the fine-tuning of a T5 model as a baseline, using the **ParaDetox dataset**. This model will be evaluated and used as a comparison to LoRA and PEFT fine-tuned models, which will be implemented next.

## Repository Structure

```bash
txt_detox/
├── notebooks
│   ├── detoxification_dataset.ipynb       # Dataset
│   ├── models                             # Folder containing saved model checkpoints
│   │   └── t5-baseline                    # Checkpoint for the fine-tuned T5 model
│   │       └── checkpoint-3645            # Model files (config, tokenizer, state, etc.)
│   └── t5_detoxification_baseline.ipynb   # Fine-tuning the T5 model for detoxification
└── references
    └── references.txt                     # Literature and resources used in the project
```

## Dataset

- **Source**: Hugging Face’s `ParaDetox` dataset.
- **Link**: [ParaDetox Dataset](https://huggingface.co/datasets/s-nlp/paradetox)
  
This dataset provides toxic and non-toxic pairs of sentences that help in training models for detoxification.

## Models

- **Baseline Model**: 
  - Fine-tuned **T5 model** for text detoxification.
  - Checkpoints and model configurations are stored in the `models/t5-baseline/` directory.
  
- **Next Steps**: 
  - Implement and fine-tune using **LoRA** and **PEFT** for efficient fine-tuning with fewer parameters.

## Installation and Setup

1. Clone the repository:

```bash
git clone https://github.com/khushpatel2002/pmldl-proj
cd pmldl-proj
```

2. Set up a Python virtual environment:

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

3. Navigate to the `notebooks/` folder and run the Jupyter notebooks to explore the dataset and fine-tune the model:

```bash
jupyter notebook
```

## Usage

You can train the baseline T5 model using the `t5_detoxification_baseline.ipynb` notebook. This notebook handles model fine-tuning, and saving model checkpoints. Check the `models/t5-baseline/checkpoint-3645/` directory for the saved model and tokenizer configurations.

## References

All relevant research papers, blog posts, and datasets used in this project can be found in the `references/references.txt` file.